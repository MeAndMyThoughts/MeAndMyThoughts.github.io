# Cross Entropy Loss 交叉熵损失
## Soft Max 归一化指数函数
- 把大的数据的影响扩大：exp(x)
- 归一化：除以sum_i(exp(x_i))
## Cross Entropy 交叉熵损失
检查预测结果的label的softmax数值（所有softmax中最大的数，0-1之间）和预测数值（1）之间的差距：log(softmax(x))，越接近1则log绝对值越小
若要检测所有的label的softmax数值（0-1之间）和预测数值（0或1）之间的差距：，把预测结果为0的变成1（softmax'=1-softmax, label'=1-label=1-0=1），预测结果为1的不变，然后sum(log(softmax'(x)))
