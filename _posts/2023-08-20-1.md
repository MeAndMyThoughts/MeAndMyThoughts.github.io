---
tags: [AI]
---
# Cross Entropy Loss 交叉熵损失

## Soft Max 归一化指数函数

- 把大的数据的影响扩大：exp(x)
- 归一化：除以sum_i(exp(x_i))
  
## Cross Entropy 交叉熵损失

a. 检查预测结果的label的softmax数值（所有softmax中最大的数，0-1之间）和预测数值（1）之间的差距：log(softmax(x))，越接近1则log绝对值越小

b. 检测所有的label的softmax数值（0-1之间）和预测数值（0或1）之间的差距：，把预测结果为0的变成1（softmax'=1-softmax, label'=1-label=1-0=1），预测结果为1的不变，然后sum(log(softmax'(x)))
